services:
  feedforward:
    build: .
    container_name: feedforward-app
    ports:
      - "5001:5001"  # Expose on all interfaces for Caddy proxy
    volumes:
      # Persist data directory (using named volume for better permissions)
      - feedforward-data:/app/data
      # Mount .env file for API keys
      - ./.env:/app/.env:ro
      # Logs volume
      - feedforward-logs:/app/logs
    environment:
      - APP_DOMAIN=https://feedforward.serveur.au
      - SECRET_KEY=${SECRET_KEY:-change-me-in-production}
      # Email configuration
      - SMTP_SERVER=${SMTP_SERVER:-smtp.gmail.com}
      - SMTP_PORT=${SMTP_PORT:-587}
      - SMTP_USER=${SMTP_USER}
      - SMTP_PASSWORD=${SMTP_PASSWORD}
      - SMTP_FROM=${SMTP_FROM:-noreply@feedforward.local}
      # Admin account
      - ADMIN_EMAIL=${ADMIN_EMAIL:-admin@example.com}
      - ADMIN_PASSWORD=${ADMIN_PASSWORD:-Admin123!}
      # LLM API Keys (optional - will use mock feedback if not provided)
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - GROQ_API_KEY=${GROQ_API_KEY}
      - OLLAMA_API_BASE=${OLLAMA_API_BASE}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001/"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 40s

  # Optional: Ollama for local LLM
  ollama:
    image: ollama/ollama:latest
    container_name: feedforward-ollama
    profiles: ["with-ollama"]  # Only start if explicitly requested
    ports:
      - "127.0.0.1:11434:11434"  # Only bind to localhost, not exposed to internet
    volumes:
      - ollama-data:/root/.ollama
    restart: unless-stopped
    # Automatically pull a model on startup
    entrypoint: ["/bin/sh", "-c"]
    command: |
      "ollama serve & 
      sleep 5 && 
      ollama pull llama3.2 && 
      tail -f /dev/null"

volumes:
  feedforward-data:
    driver: local
  feedforward-logs:
    driver: local
  ollama-data:
    driver: local